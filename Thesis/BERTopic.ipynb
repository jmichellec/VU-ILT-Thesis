{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b780c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6739615",
    "outputId": "a1e957ec-a102-4564-c669-da776c27ace3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7327888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bertopic(df_col, umap_model, embedding_model=None, dv=None):\n",
    "    if dv is None:\n",
    "        topic_model = BERTopic(umap_model=umap_model, embedding_model=embedding_model)\n",
    "        topics, probs = topic_model.fit_transform(df_col.tolist())\n",
    "    else: \n",
    "        topic_model = BERTopic(umap_model=umap_model)\n",
    "        topics, probs = topic_model.fit_transform(df_col.tolist(), dv)\n",
    "    return topic_model, topics, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2v_d2v(df_col):\n",
    "    doc2vec_args = {\"vector_size\": 300,\n",
    "                    \"min_count\": 1,\n",
    "                    \"window\": 15,\n",
    "                    \"sample\": 1e-5,\n",
    "                    \"negative\": 0,\n",
    "                    \"hs\": 1,\n",
    "                    \"epochs\": 40,\n",
    "                    \"dm\": 0,\n",
    "                    \"dbow_words\": 1}\n",
    "\n",
    "    train_corpus = [TaggedDocument(simple_preprocess(strip_tags(doc), deacc=True), [i]) for i, doc in enumerate(df_col)]\n",
    "\n",
    "    doc2vec_args[\"documents\"] = train_corpus\n",
    "\n",
    "    model = Doc2Vec(**doc2vec_args)\n",
    "\n",
    "    model.save(\"embedding_models/d2v.model\")\n",
    "    \n",
    "    print(\"Model Saved\")\n",
    "    return model\n",
    "\n",
    "def get_doc_vecs(d2v_model):\n",
    "    dv = d2v_model.docvecs\n",
    "    dv.save(\"embedding_models/d2v.docvectors\")\n",
    "    return dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence_file(coherences, file_name):\n",
    "    path = 'evaluation/' + file_name + '.txt'\n",
    "    textfile = open(path, 'w')\n",
    "    for i, coherence in enumerate(coherences):\n",
    "        textfile.write(str(i) + ': ' + str(coherence) + \"\\n\")\n",
    "    textfile.close()\n",
    "    print('Saved to ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb93342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_topic_words(topic_model, docs, topics):\n",
    "    documents = pd.DataFrame({\"Document\": docs,\n",
    "                              \"ID\": range(len(docs)),\n",
    "                              \"Topic\": topics})\n",
    "    documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "    cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "    # Extract vectorizer and analyzer from BERTopic\n",
    "    vectorizer = topic_model.vectorizer_model\n",
    "    analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "    # Extract features for Topic Coherence evaluation\n",
    "    words = vectorizer.get_feature_names()\n",
    "    tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "    dictionary = Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "                   for topic in range(len(set(topics))-1)]\n",
    "\n",
    "    return topic_words, tokens, corpus, dictionary\n",
    "\n",
    "def evaluation(topic_words, data, dictionary, corpus):\n",
    "    coherence_model = CoherenceModel(topics= topic_words, texts=data, dictionary=dictionary, coherence='c_v')\n",
    "    coherence = coherence_model.get_coherence()\n",
    "#     perplexity = model.log_perplexity(corpus)\n",
    "    return coherence#, perplexity\n",
    "\n",
    "def hyperparameter_tuning(df, name, embedding_model=None, dv=None):\n",
    "    # Change depending on models one wants to check\n",
    "    umaps = [UMAP(n_neighbors=5, n_components=5, metric='cosine', low_memory=False, random_state=42),\n",
    "             UMAP(n_neighbors=10, n_components=5, metric='cosine', low_memory=False, random_state=42),\n",
    "             UMAP(n_neighbors=15, n_components=5, metric='cosine', low_memory=False, random_state=42),\n",
    "             UMAP(n_neighbors=5, n_components=10, metric='cosine', low_memory=False, random_state=42),\n",
    "             UMAP(n_neighbors=10, n_components=10, metric='cosine', low_memory=False, random_state=42),\n",
    "             UMAP(n_neighbors=15, n_components=10, metric='cosine', low_memory=False, random_state=42),\n",
    "             ]\n",
    "    coherences = []\n",
    "    models = []\n",
    "#     perplexities = []\n",
    "    num = 0\n",
    "    for umap in umaps:\n",
    "        model, topics, probs = run_bertopic(df['preprocessed_hlead'], umap_model=umap, embedding_model=embedding_model, dv=dv)\n",
    "        bertopic_model = [model, topics, probs]\n",
    "        models.append([model, topics, probs])\n",
    "        topic_words, tokens, corpus, dictionary = prep_topic_words(model, df['preprocessed_hlead'], topics)\n",
    "        coherence = evaluation(topic_words, tokens, dictionary, corpus)\n",
    "        coherences.append(coherence)\n",
    "#         perplexities.append(perplexity)\n",
    "        print(\"Finished evaluating model \" + str(num))\n",
    "        path = 'models/' + name + '_' + str(num) + '.pkl'\n",
    "        pickle.dump(bertopic_model, open(path, 'wb'))\n",
    "        num += 1\n",
    "    return coherences, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/complete-clean-preprocessed-data-2010-2020.tsv', sep='\\t')\n",
    "\n",
    "# d2v_model = t2v_d2v(df['preprocessed_hlead'])\n",
    "# dv = get_doc_vecs(d2v_model)\n",
    "\n",
    "# doc2vec model\n",
    "doc_vectors = KeyedVectors.load(\"embedding_models/d2v.docvectors\").vectors_docs\n",
    "# fine-tuned BERT model 256 tokens\n",
    "trained_model_256 = AutoModelForMaskedLM.from_pretrained(\"embedding_models/robbert-v2-dutch-base-finetuned-model/checkpoint-5000\")\n",
    "# fine-tuned BERT model 128 tokens\n",
    "trained_model_128 = AutoModelForMaskedLM.from_pretrained(\"embedding_models/robbert-v2-dutch-base-finetuned-model/checkpoint-10000\")\n",
    "# vanilla BERT model\n",
    "vanilla_model = AutoModelForMaskedLM.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e749494",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_coherences, dv_models = hyperparameter_tuning(df, 'dv', dv=doc_vectors)\n",
    "coherence_file(dv_coherences, 'dv_models_coherences')\n",
    "best_dv = dv_models[np.argmax(dv_coherences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18333fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_128_coherences, trained_model_128_models = hyperparameter_tuning(df, 'trained_model_128', embedding_model=trained_model_128)\n",
    "coherence_file(trained_model_128_coherences, '128_models_coherences')\n",
    "best_trained_model_128 = trained_model_128_models[np.argmax(trained_model_128_coherences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed66c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_256_coherences, trained_model_256_models = hyperparameter_tuning(df, 'trained_model_256', embedding_model=trained_model_256)\n",
    "coherence_file(trained_model_256_coherences, '256_models_coherences')\n",
    "best_trained_model_256 = trained_model_256_models[np.argmax(trained_model_256_coherences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dbab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_coherences, vanilla_models = hyperparameter_tuning(df, 'vanilla', embedding_model=vanilla_model)\n",
    "coherence_file(vanilla_coherences, 'vanilla_coherences')\n",
    "best_vanilla = vanilla_models[np.argmax(vanilla_coherences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1520dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_vanilla, topics_vanilla, probs_vanilla = best_vanilla\n",
    "\n",
    "topic_model_vanilla.get_topic_info()\n",
    "topic_model_vanilla.visualize_topics() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32aa6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_256, topics_256, probs_256 = best_trained_model_256\n",
    "\n",
    "topic_model_256.get_topic_info()\n",
    "topic_model_256.visualize_topics() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff97b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_128, topics_128, probs_128 = best_trained_model_128\n",
    "\n",
    "topic_model_128.get_topic_info()\n",
    "topic_model_128.visualize_topics() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a59dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_dv, topics_dv, probs_dv = best_dv\n",
    "topic_model_dv.get_topic_info()\n",
    "topic_model_dv.visualize_topics() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = topic_model_vanilla.visualize_barchart()\n",
    "fig.write_html('test.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f60bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "?topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb732bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('Topic ' + str(i) + ':')\n",
    "    print(topic_model.get_topic(i))\n",
    "    print(\" \")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CODE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
