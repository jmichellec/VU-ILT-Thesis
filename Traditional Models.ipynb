{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3ff147",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6739615",
    "outputId": "a1e957ec-a102-4564-c669-da776c27ace3"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.nmf import Nmf\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import pprint\n",
    "import spacy\n",
    "from ast import literal_eval\n",
    "import re\n",
    "\n",
    "from tabulate import tabulate\n",
    "from texttable import Texttable\n",
    "import latextable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2936fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_data(prep_tokenized, nlp):\n",
    "    \"\"\"\n",
    "    Create lemmatised tokens column from tokenised text\n",
    "    \"\"\"\n",
    "    lemmatized = []\n",
    "    for text in prep_tokenized:\n",
    "        doc = nlp(' '.join(text))\n",
    "        lemmatized.append([token.lemma_ for token in doc])\n",
    "    return lemmatized\n",
    "\n",
    "def tokenize_clean(texts):\n",
    "    \"\"\" \n",
    "    Clean text, tokenise, remove punctuation, deaccentuate, lowercase and min-max length (tokens)\n",
    "    \"\"\"\n",
    "    for text in texts:\n",
    "        yield(simple_preprocess(str(text), deacc=True, min_len=1, max_len=100))  # removes punctuation, lowercases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8256bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create lemmatised column\n",
    "\"\"\"\n",
    "# nlp = spacy.load('nl_core_news_sm')\n",
    "\n",
    "# df = pd.read_csv('data/complete-clean-preprocessed-data-2010-2020-1.tsv', sep='\\t')\n",
    "# tokenized = tokenize_clean(df['preprocessed_hlead'].to_list())\n",
    "# lemmatized_tokenized = lemmatize_data(tokenized, nlp)\n",
    "# df['lemmatized_tokenized'] = lemmatized_tokenized\n",
    "# df.to_csv('data/complete-clean-preprocessed-data-2010-2020-1.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5967dbf",
   "metadata": {
    "id": "9d88245c"
   },
   "outputs": [],
   "source": [
    "def doc_term_matrix(data):\n",
    "    \"\"\"\n",
    "    Create document-term matrix from lemmatised tokens column\n",
    "    \"\"\"\n",
    "    lemmatized_tokenized = data.tolist()\n",
    "    dictionary = Dictionary(lemmatized_tokenized)\n",
    "    \n",
    "    # Reove tokens that appear in 90% of texts\n",
    "    dictionary.filter_extremes(no_below=1, no_above=0.9)\n",
    "\n",
    "    # bag-of-words\n",
    "    corpus = [dictionary.doc2bow(text) for text in lemmatized_tokenized]\n",
    "    \n",
    "    # tf-idf\n",
    "    tfidf = TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    print(\"Finished doc_term_matrix\")\n",
    "    return corpus_tfidf, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b6480fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, data, dictionary, corpus):\n",
    "    \"\"\"\n",
    "    calculate coherence score of model / or perplexity (unused in thesis)\n",
    "    \"\"\"\n",
    "    coherence_model = CoherenceModel(model=model, texts=data, dictionary=dictionary, coherence='c_v')\n",
    "    coherence = coherence_model.get_coherence()\n",
    "#     perplexity = model.log_perplexity(corpus)\n",
    "    return coherence#, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d36e60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(data, model, dictionary, corpus, name):\n",
    "    \"\"\"\n",
    "    hyperprarmeter tuning on number of topics; each model is saved into a folder\n",
    "    \"\"\"\n",
    "    # Change depending on models one wants to check\n",
    "    models = [model(corpus, num_topics=5, id2word = dictionary, passes=20, random_state=123),\n",
    "             model(corpus, num_topics=10, id2word = dictionary, passes=20, random_state=123),\n",
    "             model(corpus, num_topics=20, id2word = dictionary, passes=20, random_state=123),\n",
    "             model(corpus, num_topics=40, id2word = dictionary, passes=20, random_state=123),\n",
    "             ]\n",
    "    coherences = []\n",
    "#     perplexities = []\n",
    "    num = 0\n",
    "    for model in models:\n",
    "        coherence = evaluation(model, data, dictionary, corpus)\n",
    "        coherences.append(coherence)\n",
    "#         perplexities.append(perplexity)\n",
    "        print(\"Finished evaluating model \" + str(num))\n",
    "        path = 'models/' + name + '_' + str(num) + '.pkl'\n",
    "        pickle.dump(model, open(path, 'wb'))\n",
    "        num += 1\n",
    "    return coherences, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6091e24a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cc5f9ae",
    "outputId": "ea75f14b-9d60-40c4-9ec5-d17b64da3182"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished doc_term_matrix\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/complete-clean-preprocessed-data-2010-2020-1.tsv', sep='\\t', converters={'lemmatized_tokenized': literal_eval})\n",
    "data = df['lemmatized_tokenized']\n",
    "corpus_tfidf, corpus, dictionary = doc_term_matrix(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "510be002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished evaluating model 0\n",
      "Finished evaluating model 1\n",
      "Finished evaluating model 2\n",
      "Finished evaluating model 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.032*\"shell\" + 0.020*\"raffinaderij\" + 0.016*\"atm\" + 0.011*\"prinses\" + 0.008*\"olie\" + 0.008*\"nigeria\" + 0.007*\"hydraulisch\" + 0.007*\"matras\" + 0.006*\"beatrix\" + 0.005*\"harmsen\"'),\n",
       " (1,\n",
       "  '0.017*\"club\" + 0.016*\"baan\" + 0.012*\"meierijstad\" + 0.010*\"new\" + 0.009*\"circuit\" + 0.009*\"buitenring\" + 0.008*\"york\" + 0.008*\"stein\" + 0.007*\"voertuig\" + 0.007*\"ijsbaan\"'),\n",
       " (2,\n",
       "  '0.028*\"caverne\" + 0.007*\"amtsvenn\" + 0.005*\"obc\" + 0.003*\"lutte\" + 0.003*\"overbetuw\" + 0.002*\"dhg\" + 0.002*\"kadernota\" + 0.001*\"hef\" + 0.001*\"nachricht\" + 0.001*\"westfalisch\"'),\n",
       " (3,\n",
       "  '0.021*\"dat\" + 0.018*\"te\" + 0.018*\"voor\" + 0.016*\"zijn\" + 0.014*\"niet\" + 0.014*\"je\" + 0.013*\"die\" + 0.013*\"worden\" + 0.012*\"met\" + 0.012*\"als\"'),\n",
       " (4,\n",
       "  '0.092*\"b\" + 0.084*\"c\" + 0.079*\"a\" + 0.025*\"v\" + 0.020*\"welk\" + 0.016*\"arnhem\" + 0.014*\"nijmeg\" + 0.009*\"stadion\" + 0.008*\"nijmeegs\" + 0.005*\"hoe\"'),\n",
       " (5,\n",
       "  '0.005*\"dorpshart\" + 0.005*\"leimuid\" + 0.004*\"jerrycan\" + 0.004*\"schreuder\" + 0.003*\"comol\" + 0.002*\"rivierslib\" + 0.002*\"bruid\" + 0.002*\"bliemer\" + 0.002*\"leimuiden\" + 0.001*\"bruidegom\"'),\n",
       " (6,\n",
       "  '0.058*\"brand\" + 0.032*\"moerdijk\" + 0.028*\"chemie\" + 0.023*\"pack\" + 0.012*\"valkenswaard\" + 0.008*\"bluswater\" + 0.004*\"westfrisiaweg\" + 0.004*\"enkhuizerzand\" + 0.004*\"eurocircuit\" + 0.004*\"afbranden\"'),\n",
       " (7,\n",
       "  '0.022*\"nijhoff\" + 0.019*\"begraafplaats\" + 0.007*\"ommen\" + 0.004*\"hattemerbroek\" + 0.003*\"grindmaatschappij\" + 0.003*\"weggeschept\" + 0.002*\"zuiderzeehav\" + 0.002*\"dunnewind\" + 0.002*\"axel\" + 0.002*\"knoeien\"'),\n",
       " (8,\n",
       "  '0.002*\"cat\" + 0.001*\"wonderland\" + 0.000*\"isoo\" + 0.000*\"cate\" + 0.000*\"catalogusbeeld\" + 0.000*\"madelon\" + 0.000*\"stegeman\" + 0.000*\"baroktuin\" + 0.000*\"weerspiegelen\" + 0.000*\"tolweg\"'),\n",
       " (9,\n",
       "  '0.033*\"x\" + 0.010*\"lent\" + 0.006*\"enk\" + 0.005*\"waalbrug\" + 0.004*\"kasteeltuin\" + 0.002*\"duiveland\" + 0.002*\"overflakkee\" + 0.001*\"goere\" + 0.001*\"lents\" + 0.001*\"vierdaags\"'),\n",
       " (10,\n",
       "  '0.024*\"worden\" + 0.021*\"voor\" + 0.016*\"met\" + 0.016*\"aan\" + 0.015*\"te\" + 0.013*\"dat\" + 0.013*\"er\" + 0.010*\"gemeente\" + 0.009*\"zijn\" + 0.009*\"die\"'),\n",
       " (11,\n",
       "  '0.016*\"water\" + 0.015*\"te\" + 0.015*\"die\" + 0.011*\"dijk\" + 0.010*\"zijn\" + 0.010*\"om\" + 0.008*\"plant\" + 0.008*\"uit\" + 0.008*\"met\" + 0.008*\"dat\"'),\n",
       " (12,\n",
       "  '0.019*\"dat\" + 0.018*\"zijn\" + 0.016*\"met\" + 0.016*\"ik\" + 0.014*\"te\" + 0.013*\"die\" + 0.012*\"maar\" + 0.012*\"er\" + 0.012*\"niet\" + 0.011*\"hebben\"'),\n",
       " (13,\n",
       "  '0.023*\"asbestdak\" + 0.014*\"glasvezel\" + 0.011*\"tubbergen\" + 0.011*\"hilversum\" + 0.011*\"dak\" + 0.010*\"anna\" + 0.009*\"hoeve\" + 0.009*\"tynaarlo\" + 0.008*\"buitengebied\" + 0.008*\"dinkelland\"'),\n",
       " (14,\n",
       "  '0.033*\"coa\" + 0.021*\"zwembad\" + 0.017*\"vlaams\" + 0.014*\"apel\" + 0.012*\"oord\" + 0.012*\"kerk\" + 0.011*\"museum\" + 0.009*\"bad\" + 0.008*\"groev\" + 0.008*\"gouda\"'),\n",
       " (15,\n",
       "  '0.027*\"schaliegas\" + 0.024*\"gas\" + 0.014*\"naar\" + 0.011*\"energie\" + 0.011*\"winnen\" + 0.011*\"boren\" + 0.010*\"winning\" + 0.009*\"water\" + 0.008*\"nederland\" + 0.008*\"olie\"'),\n",
       " (16,\n",
       "  '0.030*\"dat\" + 0.016*\"zijn\" + 0.015*\"hebben\" + 0.015*\"hij\" + 0.014*\"heeft\" + 0.014*\"voor\" + 0.014*\"te\" + 0.013*\"niet\" + 0.013*\"bedrijf\" + 0.013*\"gemeente\"'),\n",
       " (17,\n",
       "  '0.031*\"dat\" + 0.016*\"niet\" + 0.016*\"grond\" + 0.016*\"te\" + 0.016*\"zijn\" + 0.015*\"er\" + 0.014*\"worden\" + 0.012*\"die\" + 0.012*\"voor\" + 0.012*\"met\"'),\n",
       " (18,\n",
       "  '0.019*\"worden\" + 0.017*\"jaar\" + 0.015*\"met\" + 0.012*\"voor\" + 0.012*\"grondwater\" + 0.011*\"dat\" + 0.011*\"miljoen\" + 0.010*\"provincie\" + 0.010*\"die\" + 0.010*\"vervuilen\"'),\n",
       " (19,\n",
       "  '0.084*\"bedrijf\" + 0.035*\"grondverzet\" + 0.034*\"klant\" + 0.030*\"machine\" + 0.027*\"werven\" + 0.024*\"transport\" + 0.018*\"bv\" + 0.017*\"infra\" + 0.016*\"agrarisch\" + 0.016*\"medewerker\"')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter tune the number of topics using coherence\n",
    "coherences_lda, lda_models = hyperparameter_tuning(data, LdaModel, dictionary, corpus, 'lda_model')\n",
    "# Get index of highest coherence value, can also be changed to perplexity\n",
    "best_model_index_lda = np.argmax(coherences_lda)\n",
    "lda_model = lda_models[best_model_index_lda]\n",
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f7a8734e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "0K0gBl9CNhdl",
    "outputId": "5e83633b-16a9-44a1-eea4-9c811cad0bf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished evaluating model 0\n",
      "Finished evaluating model 1\n",
      "Finished evaluating model 2\n",
      "Finished evaluating model 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.073*\"dat\" + 0.070*\"ook\" + 0.037*\"er\" + 0.036*\"zijn\" + 0.034*\"met\" + 0.032*\"worden\" + 0.025*\"uit\" + 0.014*\"al\" + 0.013*\"te\" + 0.013*\"deze\"'),\n",
       " (1,\n",
       "  '0.044*\"of\" + 0.024*\"als\" + 0.019*\"dan\" + 0.016*\"niet\" + 0.016*\"ook\" + 0.014*\"die\" + 0.013*\"tot\" + 0.010*\"kan\" + 0.009*\"deze\" + 0.009*\"over\"'),\n",
       " (2,\n",
       "  '0.080*\"die\" + 0.049*\"voor\" + 0.038*\"te\" + 0.020*\"om\" + 0.019*\"naar\" + 0.018*\"zijn\" + 0.017*\"er\" + 0.015*\"uit\" + 0.013*\"over\" + 0.012*\"worden\"'),\n",
       " (3,\n",
       "  '0.037*\"met\" + 0.036*\"was\" + 0.034*\"die\" + 0.025*\"nog\" + 0.023*\"zijn\" + 0.019*\"hebben\" + 0.016*\"er\" + 0.014*\"werd\" + 0.010*\"grond\" + 0.010*\"nieuw\"'),\n",
       " (4,\n",
       "  '0.063*\"je\" + 0.033*\"te\" + 0.027*\"zijn\" + 0.021*\"gebrek\" + 0.018*\"woning\" + 0.015*\"bij\" + 0.014*\"of\" + 0.013*\"als\" + 0.013*\"aan\" + 0.012*\"verkoper\"'),\n",
       " (5,\n",
       "  '0.074*\"we\" + 0.028*\"met\" + 0.018*\"zijn\" + 0.017*\"maar\" + 0.016*\"gaan\" + 0.015*\"om\" + 0.015*\"die\" + 0.013*\"naar\" + 0.012*\"te\" + 0.010*\"niet\"'),\n",
       " (6,\n",
       "  '0.074*\"dat\" + 0.060*\"als\" + 0.049*\"te\" + 0.019*\"voor\" + 0.016*\"niet\" + 0.016*\"ze\" + 0.013*\"geen\" + 0.012*\"worden\" + 0.012*\"hebben\" + 0.011*\"om\"'),\n",
       " (7,\n",
       "  '0.075*\"we\" + 0.040*\"er\" + 0.022*\"dat\" + 0.022*\"worden\" + 0.018*\"grond\" + 0.017*\"nu\" + 0.016*\"hebben\" + 0.016*\"afval\" + 0.014*\"was\" + 0.014*\"daar\"'),\n",
       " (8,\n",
       "  '0.043*\"zijn\" + 0.039*\"niet\" + 0.034*\"nog\" + 0.019*\"aan\" + 0.018*\"geen\" + 0.017*\"wel\" + 0.015*\"te\" + 0.015*\"werven\" + 0.012*\"hij\" + 0.011*\"dan\"'),\n",
       " (9,\n",
       "  '0.045*\"bij\" + 0.044*\"te\" + 0.022*\"dijk\" + 0.018*\"om\" + 0.016*\"water\" + 0.009*\"zullen\" + 0.008*\"over\" + 0.008*\"was\" + 0.008*\"werd\" + 0.007*\"waterschap\"'),\n",
       " (10,\n",
       "  '0.071*\"dat\" + 0.043*\"zullen\" + 0.029*\"voor\" + 0.025*\"niet\" + 0.023*\"willen\" + 0.015*\"naar\" + 0.014*\"over\" + 0.012*\"plan\" + 0.010*\"woning\" + 0.010*\"komen\"'),\n",
       " (11,\n",
       "  '0.065*\"aan\" + 0.063*\"die\" + 0.045*\"niet\" + 0.024*\"al\" + 0.022*\"hij\" + 0.021*\"er\" + 0.018*\"komen\" + 0.017*\"worden\" + 0.012*\"maar\" + 0.011*\"nu\"'),\n",
       " (12,\n",
       "  '0.031*\"of\" + 0.017*\"nederland\" + 0.016*\"veel\" + 0.011*\"duurzaam\" + 0.010*\"lichaam\" + 0.009*\"worden\" + 0.008*\"begraven\" + 0.008*\"bovendien\" + 0.008*\"kist\" + 0.007*\"wel\"'),\n",
       " (13,\n",
       "  '0.081*\"ik\" + 0.043*\"met\" + 0.025*\"mijn\" + 0.024*\"ben\" + 0.018*\"jaar\" + 0.010*\"hebben\" + 0.009*\"heb\" + 0.008*\"over\" + 0.008*\"man\" + 0.008*\"kind\"'),\n",
       " (14,\n",
       "  '0.064*\"grond\" + 0.026*\"onderzoek\" + 0.023*\"gemeente\" + 0.020*\"bedrijf\" + 0.018*\"dat\" + 0.018*\"ook\" + 0.017*\"heeft\" + 0.013*\"al\" + 0.009*\"geen\" + 0.009*\"vervuilen\"'),\n",
       " (15,\n",
       "  '0.059*\"a\" + 0.057*\"c\" + 0.056*\"b\" + 0.020*\"welk\" + 0.010*\"bij\" + 0.008*\"jaar\" + 0.008*\"was\" + 0.007*\"hoe\" + 0.007*\"wat\" + 0.007*\"vraag\"'),\n",
       " (16,\n",
       "  '0.044*\"te\" + 0.035*\"door\" + 0.024*\"met\" + 0.011*\"tot\" + 0.010*\"bodem\" + 0.010*\"jaar\" + 0.009*\"om\" + 0.007*\"groot\" + 0.007*\"zo\" + 0.007*\"water\"'),\n",
       " (17,\n",
       "  '0.072*\"te\" + 0.067*\"om\" + 0.024*\"gaan\" + 0.021*\"er\" + 0.017*\"komen\" + 0.014*\"al\" + 0.014*\"gemeente\" + 0.013*\"plan\" + 0.012*\"willen\" + 0.012*\"terrein\"'),\n",
       " (18,\n",
       "  '0.076*\"niet\" + 0.035*\"te\" + 0.026*\"dit\" + 0.018*\"maar\" + 0.016*\"hebben\" + 0.013*\"al\" + 0.013*\"willen\" + 0.010*\"zeggen\" + 0.009*\"met\" + 0.008*\"jaar\"'),\n",
       " (19,\n",
       "  '0.046*\"b\" + 0.038*\"v\" + 0.035*\"mr\" + 0.032*\"curator\" + 0.027*\"te\" + 0.022*\"a\" + 0.018*\"j\" + 0.016*\"arnhem\" + 0.015*\"m\" + 0.013*\"c\"'),\n",
       " (20,\n",
       "  '0.046*\"aan\" + 0.040*\"bedrijf\" + 0.020*\"hebben\" + 0.020*\"uit\" + 0.015*\"naar\" + 0.011*\"er\" + 0.011*\"als\" + 0.009*\"heeft\" + 0.008*\"meer\" + 0.007*\"gaan\"'),\n",
       " (21,\n",
       "  '0.022*\"voor\" + 0.021*\"bedrijf\" + 0.018*\"met\" + 0.017*\"zijn\" + 0.014*\"groot\" + 0.013*\"we\" + 0.012*\"veel\" + 0.012*\"nederland\" + 0.012*\"kunnen\" + 0.011*\"niet\"'),\n",
       " (22,\n",
       "  '0.132*\"zijn\" + 0.026*\"ze\" + 0.012*\"of\" + 0.012*\"meer\" + 0.010*\"waar\" + 0.010*\"hun\" + 0.009*\"worden\" + 0.009*\"veel\" + 0.009*\"mens\" + 0.008*\"jaar\"'),\n",
       " (23,\n",
       "  '0.070*\"grond\" + 0.042*\"vervuilen\" + 0.024*\"provincie\" + 0.023*\"met\" + 0.018*\"dat\" + 0.018*\"uit\" + 0.017*\"niet\" + 0.014*\"sanering\" + 0.012*\"gaan\" + 0.011*\"vervuiling\"'),\n",
       " (24,\n",
       "  '0.125*\"gemeente\" + 0.036*\"met\" + 0.011*\"heeft\" + 0.010*\"wethouder\" + 0.009*\"door\" + 0.009*\"bewoner\" + 0.009*\"willen\" + 0.008*\"volgens\" + 0.007*\"college\" + 0.006*\"deze\"'),\n",
       " (25,\n",
       "  '0.055*\"grond\" + 0.046*\"pfa\" + 0.022*\"stof\" + 0.015*\"norm\" + 0.010*\"komen\" + 0.010*\"zitten\" + 0.009*\"ze\" + 0.009*\"microgram\" + 0.008*\"waar\" + 0.008*\"zo\"'),\n",
       " (26,\n",
       "  '0.074*\"er\" + 0.027*\"dat\" + 0.027*\"zijn\" + 0.021*\"over\" + 0.017*\"geen\" + 0.016*\"provincie\" + 0.015*\"niet\" + 0.013*\"zeggen\" + 0.011*\"volgens\" + 0.011*\"vervuiling\"'),\n",
       " (27,\n",
       "  '0.100*\"voor\" + 0.070*\"met\" + 0.056*\"aan\" + 0.012*\"nieuw\" + 0.010*\"bij\" + 0.010*\"deze\" + 0.009*\"dit\" + 0.008*\"maken\" + 0.007*\"plan\" + 0.006*\"om\"'),\n",
       " (28,\n",
       "  '0.155*\"worden\" + 0.016*\"moet\" + 0.014*\"voor\" + 0.012*\"nieuw\" + 0.011*\"kan\" + 0.010*\"meter\" + 0.009*\"zullen\" + 0.008*\"door\" + 0.006*\"dit\" + 0.006*\"moeten\"'),\n",
       " (29,\n",
       "  '0.034*\"euro\" + 0.034*\"dat\" + 0.030*\"miljoen\" + 0.030*\"voor\" + 0.024*\"aan\" + 0.018*\"jaar\" + 0.013*\"nog\" + 0.010*\"dan\" + 0.010*\"tot\" + 0.009*\"gaan\"'),\n",
       " (30,\n",
       "  '0.156*\"dat\" + 0.055*\"die\" + 0.032*\"om\" + 0.025*\"zeggen\" + 0.022*\"maar\" + 0.021*\"hebben\" + 0.019*\"niet\" + 0.014*\"dan\" + 0.012*\"heeft\" + 0.011*\"daar\"'),\n",
       " (31,\n",
       "  '0.045*\"die\" + 0.030*\"ze\" + 0.017*\"bij\" + 0.014*\"aan\" + 0.013*\"ook\" + 0.012*\"als\" + 0.011*\"werd\" + 0.011*\"door\" + 0.010*\"zich\" + 0.008*\"soort\"'),\n",
       " (32,\n",
       "  '0.085*\"ik\" + 0.028*\"dat\" + 0.018*\"maar\" + 0.018*\"te\" + 0.013*\"niet\" + 0.013*\"mijn\" + 0.012*\"heb\" + 0.012*\"hij\" + 0.011*\"als\" + 0.011*\"je\"'),\n",
       " (33,\n",
       "  '0.042*\"je\" + 0.032*\"er\" + 0.024*\"met\" + 0.015*\"dan\" + 0.014*\"wat\" + 0.013*\"als\" + 0.012*\"dat\" + 0.012*\"maar\" + 0.011*\"nog\" + 0.011*\"gaan\"'),\n",
       " (34,\n",
       "  '0.036*\"bij\" + 0.036*\"jaar\" + 0.030*\"al\" + 0.030*\"komen\" + 0.020*\"dat\" + 0.016*\"er\" + 0.014*\"nog\" + 0.013*\"nieuw\" + 0.011*\"meer\" + 0.011*\"voor\"'),\n",
       " (35,\n",
       "  '0.068*\"hij\" + 0.038*\"zijn\" + 0.025*\"voor\" + 0.016*\"jaar\" + 0.011*\"er\" + 0.011*\"hem\" + 0.010*\"hebben\" + 0.009*\"te\" + 0.009*\"uit\" + 0.009*\"heeft\"'),\n",
       " (36,\n",
       "  '0.051*\"ze\" + 0.032*\"haar\" + 0.015*\"naar\" + 0.014*\"uit\" + 0.013*\"over\" + 0.010*\"heeft\" + 0.009*\"was\" + 0.007*\"huis\" + 0.007*\"gaan\" + 0.006*\"tegen\"'),\n",
       " (37,\n",
       "  '0.036*\"maar\" + 0.021*\"ook\" + 0.021*\"voor\" + 0.016*\"door\" + 0.013*\"linkebeek\" + 0.012*\"vlaams\" + 0.011*\"nederlands\" + 0.010*\"franstalig\" + 0.009*\"hun\" + 0.007*\"niet\"'),\n",
       " (38,\n",
       "  '0.026*\"uur\" + 0.023*\"tot\" + 0.011*\"aan\" + 0.011*\"bij\" + 0.008*\"worden\" + 0.008*\"september\" + 0.007*\"met\" + 0.007*\"kan\" + 0.007*\"via\" + 0.006*\"den\"'),\n",
       " (39,\n",
       "  '0.063*\"hebben\" + 0.030*\"je\" + 0.022*\"al\" + 0.019*\"ook\" + 0.019*\"was\" + 0.017*\"niet\" + 0.012*\"ze\" + 0.011*\"we\" + 0.010*\"kunnen\" + 0.010*\"maar\"')]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter tune the number of topics using coherence\n",
    "coherences_nmf, nmf_models = hyperparameter_tuning(data, Nmf, dictionary, corpus, 'nmf_model')\n",
    "# Get index of highest coherence value, can also be changed to perplexity\n",
    "best_model_index_nmf = np.argmax(coherences_nmf)\n",
    "nmf_model = nmf_models[best_model_index_nmf]\n",
    "nmf_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8b3e4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished evaluating model 0\n",
      "Finished evaluating model 1\n",
      "Finished evaluating model 2\n",
      "Finished evaluating model 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*\"ik\" + 0.002*\"we\" + 0.002*\"gemeente\" + 0.002*\"provincie\" + 0.002*\"hij\" + 0.002*\"grond\" + 0.002*\"je\" + 0.002*\"terrein\" + 0.001*\"ze\" + 0.001*\"onderzoek\"'),\n",
       " (1,\n",
       "  '0.001*\"asbestdak\" + 0.001*\"staalslak\" + 0.001*\"staalslakken\" + 0.000*\"hoeve\" + 0.000*\"tata\" + 0.000*\"vredenburg\" + 0.000*\"hvc\" + 0.000*\"waadhoek\" + 0.000*\"ballast\" + 0.000*\"nedam\"'),\n",
       " (2,\n",
       "  '0.003*\"drugsafval\" + 0.002*\"wml\" + 0.001*\"drug\" + 0.001*\"dader\" + 0.001*\"meierijstad\" + 0.001*\"rijsenhout\" + 0.001*\"xtc\" + 0.001*\"synthetisch\" + 0.001*\"bentum\" + 0.001*\"sulfaat\"'),\n",
       " (3,\n",
       "  '0.002*\"schaliegas\" + 0.001*\"coa\" + 0.001*\"recyclingbedrijf\" + 0.001*\"tankstation\" + 0.001*\"roermond\" + 0.001*\"vitens\" + 0.001*\"afvalstof\" + 0.001*\"haarlemmermeer\" + 0.001*\"miljard\" + 0.001*\"eindhoven\"'),\n",
       " (4,\n",
       "  '0.001*\"griftpark\" + 0.001*\"aldi\" + 0.000*\"putman\" + 0.000*\"abn\" + 0.000*\"amro\" + 0.000*\"geldrop\" + 0.000*\"mierlo\" + 0.000*\"willemsoord\" + 0.000*\"volgermeer\" + 0.000*\"zaans\"'),\n",
       " (5,\n",
       "  '0.001*\"culemborg\" + 0.000*\"oisterwijk\" + 0.000*\"kademuur\" + 0.000*\"venray\" + 0.000*\"nagel\" + 0.000*\"akzo\" + 0.000*\"zandwinpla\" + 0.000*\"maes\" + 0.000*\"rijksstraatweg\" + 0.000*\"vve\"'),\n",
       " (6,\n",
       "  '0.001*\"akzonobel\" + 0.001*\"componenta\" + 0.001*\"jochem\" + 0.001*\"vdl\" + 0.001*\"broekerhavenweg\" + 0.000*\"tongeren\" + 0.000*\"witteve\" + 0.000*\"zinkas\" + 0.000*\"rw\" + 0.000*\"abdk\"'),\n",
       " (7,\n",
       "  '0.001*\"linssen\" + 0.001*\"werven\" + 0.001*\"valkenburg\" + 0.001*\"strafzaak\" + 0.001*\"dekker\" + 0.001*\"bag\" + 0.001*\"meijer\" + 0.001*\"dreumel\" + 0.001*\"buitenring\" + 0.001*\"uiterwaard\"'),\n",
       " (8,\n",
       "  '0.001*\"marinier\" + 0.001*\"marinierskazerne\" + 0.001*\"sallandhof\" + 0.001*\"follow\" + 0.001*\"valkenswaard\" + 0.001*\"rijpelberg\" + 0.001*\"sodm\" + 0.000*\"korps\" + 0.000*\"vasse\" + 0.000*\"money\"'),\n",
       " (9,\n",
       "  '0.001*\"gs\" + 0.001*\"vels\" + 0.001*\"broekpolder\" + 0.001*\"nrg\" + 0.000*\"heibloem\" + 0.000*\"tritium\" + 0.000*\"sachem\" + 0.000*\"westerman\" + 0.000*\"kaagbaan\" + 0.000*\"ugcheel\"')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter tune the number of topics using coherence\n",
    "coherences_lda_tfidf, lda_models_tfidf = hyperparameter_tuning(data, LdaModel, dictionary, corpus_tfidf, 'lda_model_tfidf')\n",
    "# Get index of highest coherence value, can also be changed to perplexity\n",
    "best_model_index_lda_tfidf = np.argmax(coherences_lda_tfidf)\n",
    "lda_model_tfidf = lda_models_tfidf[best_model_index_lda_tfidf]\n",
    "lda_model_tfidf.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f327dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished evaluating model 0\n",
      "Finished evaluating model 1\n",
      "Finished evaluating model 2\n",
      "Finished evaluating model 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(16,\n",
       "  '0.008*\"we\" + 0.006*\"je\" + 0.004*\"ze\" + 0.003*\"ons\" + 0.003*\"wat\" + 0.003*\"wij\" + 0.003*\"goed\" + 0.003*\"maar\" + 0.003*\"onze\" + 0.002*\"mens\"'),\n",
       " (33,\n",
       "  '0.005*\"bedrijf\" + 0.004*\"grondverzet\" + 0.004*\"boer\" + 0.004*\"machine\" + 0.003*\"hij\" + 0.003*\"werk\" + 0.003*\"agrarisch\" + 0.002*\"werken\" + 0.002*\"uur\" + 0.002*\"klant\"'),\n",
       " (31,\n",
       "  '0.011*\"plant\" + 0.011*\"duizendknoop\" + 0.006*\"exoot\" + 0.005*\"wetenschapper\" + 0.005*\"spitsberg\" + 0.005*\"expeditie\" + 0.004*\"japans\" + 0.004*\"dier\" + 0.004*\"eiland\" + 0.004*\"soort\"'),\n",
       " (37,\n",
       "  '0.007*\"woning\" + 0.005*\"plan\" + 0.005*\"bouw\" + 0.004*\"bouwen\" + 0.004*\"appartement\" + 0.003*\"centrum\" + 0.003*\"ontwikkelaar\" + 0.003*\"huis\" + 0.002*\"nieuw\" + 0.002*\"nieuwbouw\"'),\n",
       " (32,\n",
       "  '0.029*\"ik\" + 0.010*\"mijn\" + 0.007*\"heb\" + 0.006*\"ben\" + 0.005*\"hij\" + 0.005*\"me\" + 0.003*\"je\" + 0.003*\"mij\" + 0.002*\"doornbos\" + 0.002*\"u\"'),\n",
       " (13,\n",
       "  '0.020*\"pand\" + 0.005*\"olafstraat\" + 0.004*\"gebouw\" + 0.004*\"stomerij\" + 0.004*\"verkoop\" + 0.004*\"eigenaar\" + 0.004*\"verkopen\" + 0.004*\"sint\" + 0.003*\"hij\" + 0.003*\"corporatie\"'),\n",
       " (29,\n",
       "  '0.013*\"euro\" + 0.012*\"miljoen\" + 0.008*\"kost\" + 0.005*\"extra\" + 0.005*\"kosten\" + 0.004*\"bedrag\" + 0.004*\"tegenvaller\" + 0.004*\"betalen\" + 0.004*\"ton\" + 0.003*\"project\"'),\n",
       " (25,\n",
       "  '0.023*\"ameland\" + 0.023*\"windstoot\" + 0.022*\"strandtent\" + 0.021*\"boorlocatie\" + 0.015*\"zee\" + 0.013*\"zwaar\" + 0.012*\"lauwersoog\" + 0.012*\"dijkdoorgang\" + 0.011*\"annuleren\" + 0.011*\"dijkbewaking\"'),\n",
       " (35,\n",
       "  '0.029*\"boom\" + 0.008*\"kappen\" + 0.004*\"kap\" + 0.003*\"jong\" + 0.003*\"anne\" + 0.003*\"werkzaamheid\" + 0.002*\"kastanje\" + 0.002*\"paardenkastanje\" + 0.002*\"planten\" + 0.002*\"bos\"'),\n",
       " (12,\n",
       "  '0.012*\"lichaam\" + 0.011*\"begraven\" + 0.011*\"kist\" + 0.009*\"je\" + 0.008*\"grafsteen\" + 0.008*\"cremeren\" + 0.008*\"duurzaam\" + 0.006*\"natuurbegraafplaats\" + 0.006*\"resomeren\" + 0.006*\"of\"'),\n",
       " (39,\n",
       "  '0.016*\"raad\" + 0.016*\"state\" + 0.012*\"dwangsom\" + 0.005*\"uitspraak\" + 0.005*\"gemeente\" + 0.004*\"wal\" + 0.004*\"overtreding\" + 0.004*\"opleggen\" + 0.004*\"grondnet\" + 0.004*\"euro\"'),\n",
       " (19,\n",
       "  '0.020*\"maas\" + 0.017*\"tiel\" + 0.016*\"herwijn\" + 0.011*\"waal\" + 0.009*\"munt\" + 0.006*\"schip\" + 0.006*\"eeuw\" + 0.006*\"herwijnen\" + 0.006*\"rijkswaterstaat\" + 0.006*\"dreumel\"'),\n",
       " (9,\n",
       "  '0.025*\"waterschap\" + 0.013*\"westdijk\" + 0.007*\"vallei\" + 0.006*\"thermisch\" + 0.006*\"reinigen\" + 0.005*\"spakenburg\" + 0.005*\"bunschoten\" + 0.005*\"tgg\" + 0.005*\"bunschot\" + 0.005*\"dijk\"'),\n",
       " (0,\n",
       "  '0.031*\"asbest\" + 0.006*\"speeltuin\" + 0.005*\"terrein\" + 0.005*\"onderzoek\" + 0.005*\"inspectie\" + 0.004*\"aantreffen\" + 0.004*\"vondst\" + 0.004*\"grond\" + 0.004*\"materiaal\" + 0.004*\"speelplaats\"'),\n",
       " (5,\n",
       "  '0.029*\"vat\" + 0.021*\"drugsafval\" + 0.015*\"afval\" + 0.010*\"politie\" + 0.009*\"dumpen\" + 0.006*\"brandweer\" + 0.006*\"lekken\" + 0.005*\"drug\" + 0.005*\"xtc\" + 0.005*\"opruimen\"'),\n",
       " (23,\n",
       "  '0.039*\"edelchemie\" + 0.027*\"nevel\" + 0.015*\"panheel\" + 0.011*\"rechtbank\" + 0.011*\"recyclingbedrijf\" + 0.010*\"linssen\" + 0.010*\"leo\" + 0.010*\"zoon\" + 0.009*\"justitie\" + 0.008*\"afvalstof\"'),\n",
       " (7,\n",
       "  '0.050*\"kleiput\" + 0.016*\"winterswijk\" + 0.013*\"storten\" + 0.008*\"actiegroep\" + 0.007*\"roelofs\" + 0.007*\"pfa\" + 0.007*\"abk\" + 0.006*\"partij\" + 0.005*\"behoud\" + 0.005*\"glooien\"'),\n",
       " (38,\n",
       "  '0.052*\"vink\" + 0.021*\"barneveld\" + 0.009*\"kevelam\" + 0.009*\"barnevelds\" + 0.008*\"zembla\" + 0.008*\"zand\" + 0.006*\"veenendaal\" + 0.005*\"gebruiken\" + 0.004*\"nieuwbouwwijk\" + 0.004*\"afvalbedrijf\"'),\n",
       " (6,\n",
       "  '0.036*\"pack\" + 0.034*\"chemie\" + 0.022*\"moerdijk\" + 0.020*\"brand\" + 0.010*\"bluswater\" + 0.007*\"delta\" + 0.006*\"brabant\" + 0.006*\"brabants\" + 0.005*\"bedrijf\" + 0.005*\"atm\"'),\n",
       " (20,\n",
       "  '0.041*\"pfa\" + 0.015*\"norm\" + 0.013*\"microgram\" + 0.010*\"stof\" + 0.008*\"rivm\" + 0.008*\"regel\" + 0.008*\"veldhov\" + 0.006*\"kilo\" + 0.005*\"staatssecretaris\" + 0.005*\"streng\"')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter tune the number of topics using coherence\n",
    "coherences_nmf_tfidf, nmf_models_tfidf = hyperparameter_tuning(data, Nmf, dictionary, corpus_tfidf, 'nmf_model_tfidf')\n",
    "# Get index of highest coherence value, can also be changed to perplexity\n",
    "best_model_index_nmf_tfidf = np.argmax(coherences_nmf_tfidf)\n",
    "nmf_model_tfidf = nmf_models_tfidf[best_model_index_nmf_tfidf]\n",
    "nmf_model_tfidf.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f3556c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in models with highest topic coherence score\n",
    "lda_model = pickle.load(open('models/lda_model_2.pkl', 'rb'))\n",
    "nmf_model = pickle.load(open('models/nmf_model_3.pkl', 'rb'))\n",
    "lda_model_tfidf = pickle.load(open('models/lda_model_tfidf_1.pkl', 'rb'))\n",
    "nmf_model_tfidf = pickle.load(open('models/nmf_model_tfidf_3.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "6d7b8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_lists(model, num_topics, num_words):\n",
    "    \"\"\" Get topic_ids, topic words and probabilities of a model\"\"\"\n",
    "    topic_ids = []\n",
    "    topic_words_per_topic = []\n",
    "    topic_probs_per_topic = []\n",
    "    \n",
    "    topic_list = model.print_topics(num_topics=num_topics, num_words=num_words)\n",
    "    for topic in topic_list:\n",
    "\n",
    "        topic_words = []\n",
    "\n",
    "        topic_probs = []\n",
    "\n",
    "\n",
    "        topic_id = str(topic[0])\n",
    "        topic_ids.append(topic_id)\n",
    "\n",
    "        topic_words_list = topic[1]\n",
    "\n",
    "        topic_prob_word_list = topic_words_list.split('+')\n",
    "        for topic_prob_word in topic_prob_word_list:\n",
    "            topic_prob, topic_word = topic_prob_word.split('*')\n",
    "            topic_word = re.sub(\"\\\"\", '', topic_word)\n",
    "            topic_words.append(topic_word)\n",
    "            topic_probs.append(topic_prob)\n",
    "        topic_words_per_topic.append(topic_words)\n",
    "        topic_probs_per_topic.append(topic_probs)\n",
    "    \n",
    "    return topic_ids, topic_words_per_topic, topic_probs_per_topic\n",
    "\n",
    "\n",
    "def to_latex(topic_ids, topic_words_per_topic, num_topics, caption, label):\n",
    "    \"\"\"\n",
    "    Use get_topic_lists() to get topic_ids, words and probabilities to create Latex Tables\n",
    "    \"\"\"\n",
    "    \n",
    "    # transpose\n",
    "    topic_words_per_topic_T = list(map(list, zip(*topic_words_per_topic)))\n",
    "    rows = [topic_ids]\n",
    "\n",
    "    for words in topic_words_per_topic_T:\n",
    "        rows.append(words)\n",
    "\n",
    "    table = Texttable()\n",
    "    table.set_cols_align([\"c\"] * num_topics)\n",
    "\n",
    "    table.set_deco(Texttable.HEADER | Texttable.VLINES)\n",
    "    table.add_rows(rows)\n",
    "    \n",
    "    print('\\nTexttable Latex:')\n",
    "    print(latextable.draw_latex(table, caption=caption, use_booktabs=True, label=label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "ddd88db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texttable Latex:\n",
      "\\begin{table}\n",
      "\t\\begin{center}\n",
      "\t\t\\begin{tabular}{c c c c c}\n",
      "\t\t\t\\toprule\n",
      "\t\t\t5 & 12 & 9 & 1 & 8 \\\\\n",
      "\t\t\t\\midrule\n",
      "\t\t\tdorpshart  & dat  & x  & club  & cat  \\\\\n",
      "\t\t\tleimuid  & zijn  & lent  & baan  & wonderland  \\\\\n",
      "\t\t\tjerrycan  & met  & enk  & meierijstad  & isoo  \\\\\n",
      "\t\t\tschreuder  & ik  & waalbrug  & new  & cate  \\\\\n",
      "\t\t\tcomol  & te  & kasteeltuin  & circuit  & catalogusbeeld  \\\\\n",
      "\t\t\trivierslib  & die  & duiveland  & buitenring  & madelon  \\\\\n",
      "\t\t\tbruid  & maar  & overflakkee  & york  & stegeman  \\\\\n",
      "\t\t\tbliemer  & er  & goere  & stein  & baroktuin  \\\\\n",
      "\t\t\tleimuiden  & niet  & lents  & voertuig  & weerspiegelen  \\\\\n",
      "\t\t\tbruidegom & hebben & vierdaags & ijsbaan & tolweg \\\\\n",
      "\t\t\t\\bottomrule\n",
      "\t\t\\end{tabular}\n",
      "\t\\end{center}\n",
      "\t\\caption{a}\n",
      "\t\\label{a}\n",
      "\\end{table}\n",
      "\n",
      "Texttable Latex:\n",
      "\\begin{table}\n",
      "\t\\begin{center}\n",
      "\t\t\\begin{tabular}{c c c c c}\n",
      "\t\t\t\\toprule\n",
      "\t\t\t38 & 16 & 25 & 30 & 15 \\\\\n",
      "\t\t\t\\midrule\n",
      "\t\t\tuur  & te  & grond  & dat  & a  \\\\\n",
      "\t\t\ttot  & door  & pfa  & die  & c  \\\\\n",
      "\t\t\taan  & met  & stof  & om  & b  \\\\\n",
      "\t\t\tbij  & tot  & norm  & zeggen  & welk  \\\\\n",
      "\t\t\tworden  & bodem  & komen  & maar  & bij  \\\\\n",
      "\t\t\tseptember  & jaar  & zitten  & hebben  & jaar  \\\\\n",
      "\t\t\tmet  & om  & ze  & niet  & was  \\\\\n",
      "\t\t\tkan  & groot  & microgram  & dan  & hoe  \\\\\n",
      "\t\t\tvia  & zo  & waar  & heeft  & wat  \\\\\n",
      "\t\t\tden & water & zo & daar & vraag \\\\\n",
      "\t\t\t\\bottomrule\n",
      "\t\t\\end{tabular}\n",
      "\t\\end{center}\n",
      "\t\\caption{a}\n",
      "\t\\label{a}\n",
      "\\end{table}\n",
      "\n",
      "Texttable Latex:\n",
      "\\begin{table}\n",
      "\t\\begin{center}\n",
      "\t\t\\begin{tabular}{c c c c c}\n",
      "\t\t\t\\toprule\n",
      "\t\t\t9 & 7 & 1 & 3 & 0 \\\\\n",
      "\t\t\t\\midrule\n",
      "\t\t\tgs  & linssen  & asbestdak  & schaliegas  & ik  \\\\\n",
      "\t\t\tvels  & werven  & staalslak  & coa  & we  \\\\\n",
      "\t\t\tbroekpolder  & valkenburg  & staalslakken  & recyclingbedrijf  & gemeente  \\\\\n",
      "\t\t\tnrg  & strafzaak  & hoeve  & tankstation  & provincie  \\\\\n",
      "\t\t\theibloem  & dekker  & tata  & roermond  & hij  \\\\\n",
      "\t\t\ttritium  & bag  & vredenburg  & vitens  & grond  \\\\\n",
      "\t\t\tsachem  & meijer  & hvc  & afvalstof  & je  \\\\\n",
      "\t\t\twesterman  & dreumel  & waadhoek  & haarlemmermeer  & terrein  \\\\\n",
      "\t\t\tkaagbaan  & buitenring  & ballast  & miljard  & ze  \\\\\n",
      "\t\t\tugcheel & uiterwaard & nedam & eindhoven & onderzoek \\\\\n",
      "\t\t\t\\bottomrule\n",
      "\t\t\\end{tabular}\n",
      "\t\\end{center}\n",
      "\t\\caption{a}\n",
      "\t\\label{a}\n",
      "\\end{table}\n",
      "\n",
      "Texttable Latex:\n",
      "\\begin{table}\n",
      "\t\\begin{center}\n",
      "\t\t\\begin{tabular}{c c c c c}\n",
      "\t\t\t\\toprule\n",
      "\t\t\t16 & 33 & 38 & 6 & 20 \\\\\n",
      "\t\t\t\\midrule\n",
      "\t\t\twe  & bedrijf  & vink  & pack  & pfa  \\\\\n",
      "\t\t\tje  & grondverzet  & barneveld  & chemie  & norm  \\\\\n",
      "\t\t\tze  & boer  & kevelam  & moerdijk  & microgram  \\\\\n",
      "\t\t\tons  & machine  & barnevelds  & brand  & stof  \\\\\n",
      "\t\t\twat  & hij  & zembla  & bluswater  & rivm  \\\\\n",
      "\t\t\twij  & werk  & zand  & delta  & regel  \\\\\n",
      "\t\t\tgoed  & agrarisch  & veenendaal  & brabant  & veldhov  \\\\\n",
      "\t\t\tmaar  & werken  & gebruiken  & brabants  & kilo  \\\\\n",
      "\t\t\tonze  & uur  & nieuwbouwwijk  & bedrijf  & staatssecretaris  \\\\\n",
      "\t\t\tmens & klant & afvalbedrijf & atm & streng \\\\\n",
      "\t\t\t\\bottomrule\n",
      "\t\t\\end{tabular}\n",
      "\t\\end{center}\n",
      "\t\\caption{a}\n",
      "\t\\label{a}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "# Code to create latex tables of best models\n",
    "models = [lda_model, nmf_model, lda_model_tfidf, nmf_model_tfidf]\n",
    "caption = 'a'\n",
    "label = 'a'\n",
    "for model in models:\n",
    "    topic_ids, topic_words_per_topic, topic_probs_per_topic = get_topic_lists(model, 5, 10)\n",
    "    to_latex(topic_ids, topic_words_per_topic, 5, caption, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "370e872c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.2945</td>\n",
       "      <td>dat, te, worden, er, niet, voor, zijn, met, om...</td>\n",
       "      <td>In februari 1945 verging de Duitse onderzeeboo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.5079</td>\n",
       "      <td>grond, dat, worden, zijn, er, vervuilen, met, ...</td>\n",
       "      <td>De voormalige stortplaats in de Kanaalpolder b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.7969</td>\n",
       "      <td>met, voor, zijn, we, te, die, dat, ook, aan, er</td>\n",
       "      <td>Tijdens het prinsenbal van de Peelpluimen in d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.3930</td>\n",
       "      <td>worden, voor, aan, met, er, te, gemeente, dat,...</td>\n",
       "      <td>Het plan voor de bouw van twaalf woningen op h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.6951</td>\n",
       "      <td>dat, te, worden, er, niet, voor, zijn, met, om...</td>\n",
       "      <td>De weersvoorspellingen waren goed, het zag er ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.6987</td>\n",
       "      <td>grond, dat, worden, zijn, er, vervuilen, met, ...</td>\n",
       "      <td>De sterk vervuilde grond in de Doornse Heuvelw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>grond, dat, worden, zijn, er, vervuilen, met, ...</td>\n",
       "      <td>De woonwijk het Franse Gat, grotendeels gebouw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.3555</td>\n",
       "      <td>grond, dat, worden, zijn, er, vervuilen, met, ...</td>\n",
       "      <td>Voor bewoners van de Veenendaalse wijk het Fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.2971</td>\n",
       "      <td>dat, te, worden, er, niet, voor, zijn, met, om...</td>\n",
       "      <td>Het grote (financiële) knelpunt is de aanwezig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>ik, dat, zijn, niet, hij, je, maar, ze, die, h...</td>\n",
       "      <td>Ogenschijnlijk doen de bewoners van Franse Gat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0            17.0              0.2945   \n",
       "1            1            26.0              0.5079   \n",
       "2            2            12.0              0.7969   \n",
       "3            3            10.0              0.3930   \n",
       "4            4            17.0              0.6951   \n",
       "5            5            26.0              0.6987   \n",
       "6            6            26.0              0.6600   \n",
       "7            7            26.0              0.3555   \n",
       "8            8            17.0              0.2971   \n",
       "9            9            23.0              0.5200   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  dat, te, worden, er, niet, voor, zijn, met, om...   \n",
       "1  grond, dat, worden, zijn, er, vervuilen, met, ...   \n",
       "2    met, voor, zijn, we, te, die, dat, ook, aan, er   \n",
       "3  worden, voor, aan, met, er, te, gemeente, dat,...   \n",
       "4  dat, te, worden, er, niet, voor, zijn, met, om...   \n",
       "5  grond, dat, worden, zijn, er, vervuilen, met, ...   \n",
       "6  grond, dat, worden, zijn, er, vervuilen, met, ...   \n",
       "7  grond, dat, worden, zijn, er, vervuilen, met, ...   \n",
       "8  dat, te, worden, er, niet, voor, zijn, met, om...   \n",
       "9  ik, dat, zijn, niet, hij, je, maar, ze, die, h...   \n",
       "\n",
       "                                                Text  \n",
       "0  In februari 1945 verging de Duitse onderzeeboo...  \n",
       "1  De voormalige stortplaats in de Kanaalpolder b...  \n",
       "2  Tijdens het prinsenbal van de Peelpluimen in d...  \n",
       "3  Het plan voor de bouw van twaalf woningen op h...  \n",
       "4  De weersvoorspellingen waren goed, het zag er ...  \n",
       "5  De sterk vervuilde grond in de Doornse Heuvelw...  \n",
       "6  De woonwijk het Franse Gat, grotendeels gebouw...  \n",
       "7  Voor bewoners van de Veenendaalse wijk het Fra...  \n",
       "8  Het grote (financiële) knelpunt is de aanwezig...  \n",
       "9  Ogenschijnlijk doen de bewoners van Franse Gat...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unused\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    \"\"\"\n",
    "    Code to format topics into dataframe, including keywords and topic contribution\n",
    "    \n",
    "    \"\"\"\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(lda_model[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(lda_model, corpus, df['preprocessed_hlead'])\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CODE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
